{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33c071dd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0792a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b169c2f",
   "metadata": {},
   "source": [
    "In this notebook we will consider a simple linear regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e3763",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a16cf0",
   "metadata": {},
   "source": [
    "We will be using the \"summation conventions\": when an index is repeated the summation over this index is implied:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f2996",
   "metadata": {},
   "source": [
    "$$ \n",
    "x_{ij} w_j \\equiv   \\sum_j x_{ij} w_j \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02a1aa",
   "metadata": {},
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e128f69",
   "metadata": {},
   "source": [
    "Implement function `linear(x,w,b)` that given feature matrix $\\mathbf{x}$, weights $\\mathbf{w}$ and bias $b$  returns $\\mathbf{y}$. **Hint** Use matrix multiplication operator `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc4f31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x,w,b):\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00deb1c4",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5198cd5",
   "metadata": {},
   "source": [
    "#### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45c0d1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Generate a random feature matrix $\\mathbf{x}$ witch 10000 samples and three features, such that first feature is drawn from normal distribution $\\mathcal{N}(0,1)$, second feature from  uniform distribution on interval $[0,1)$ and third from $\\mathcal{N}(1,2)$, where \n",
    "$N(\\mu,\\sigma)$ denotes normal distribution with mean $\\mu$ and standard deviation $\\sigma$. To generate random numbers you can use `numpy.random.normal` and `numpy.random.uniform` functions. To collect all features together you can use `numpy.stack` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275db2b8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Then using $\\mathbf{x}$, weights $w_{true}$  and  bias $b_{true}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68861ec5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "w_true = np.array([0.2, 0.5,-0.2])\n",
    "b_true = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9769b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "generate output $\\mathbf{y}$ assuming a normaly distributed $\\mathcal{N}(0,0.1)$ noise $\\mathbf{\\epsilon}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2288e9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ y_i =  \n",
    "x_{ij} w_j+b +\\epsilon_i \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8592f628-9f6f-4751-8713-11449ca0251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.73537593 -0.61559949 -0.65594338 ... -1.93543157 -0.29377954\n",
      " -0.58592315]\n",
      "(10000,)\n",
      "-9516.601340763587\n"
     ]
    }
   ],
   "source": [
    "x = np.stack((np.random.normal(0,1,10000),np.random.uniform(0,1,10000),np.random.normal(1,2,10000)),axis=-1)\n",
    "noise = np.random.normal(0,0.1,10000)\n",
    "y = linear(x, w_true, b_true) + noise\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965b06c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575195b5",
   "metadata": {},
   "source": [
    "#### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4365568",
   "metadata": {},
   "source": [
    "Given the means square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb502c6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ MSE(w,b|y,x) = \\frac{1}{2}\\frac{1}{N}\\sum_{i=0}^{N-1} (y_i -  x_{ij} w_j -b  )^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce517b",
   "metadata": {},
   "source": [
    "write down the python function `mse(y,x,w,b)` implementing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b6b372f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004950066381661358\n"
     ]
    }
   ],
   "source": [
    "def mse(y,x,w,b):\n",
    "    n = y.shape\n",
    "    suma = np.sum(np.power((y - x @ w - b),2))\n",
    "    #print(suma)\n",
    "    return 1/(2*n[0])*suma\n",
    "\n",
    "print(mse(y,x,w_true,b_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e9c30",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154af3c2",
   "metadata": {},
   "source": [
    "and implement functions `grad_w(y,x,w,b)` and `grad_b(y,x,w,b)` implementing those gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d3f3a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_w(y,x,w,b):\n",
    "    n = y.shape\n",
    "    return -1/n[0]*(y - x @ w - b) @ x\n",
    "\n",
    "def grad_b(y,x,w,b):\n",
    "    n = y.shape\n",
    "    suma = np.sum((y - x @ w - b))\n",
    "    return -1/n[0]*suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ec26c0-56a2-46ea-b562-c6696801df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00020756 -0.00071817 -0.00218086]\n",
      "-0.0007519068328696676\n"
     ]
    }
   ],
   "source": [
    "print(grad_w(y,x,w_true,b_true))\n",
    "print(grad_b(y,x,w_true,b_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddacf1",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913a559",
   "metadata": {},
   "source": [
    "#### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a2437",
   "metadata": {},
   "source": [
    "Implement gradient descent for linear regression. Start from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34dc29be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "0.007492174943384497\n",
      "[ 0.19947152  0.25962773 -0.2020967 ]\n",
      "-0.8698104354081398\n"
     ]
    }
   ],
   "source": [
    "w = np.asarray([0.0,0.0,0.0], dtype='float64')\n",
    "b = 1.0 \n",
    "epochs = 1000\n",
    "step = 0.1\n",
    "treshold=0.0075\n",
    "err=1\n",
    "\n",
    "for i in range(epochs):\n",
    "    w=w-step * grad_w(y,x,w,b)\n",
    "    b=b-step * grad_b(y,x,w,b)\n",
    "    err = mse(y,x,w,b)\n",
    "    if(err<treshold):\n",
    "        print(i)\n",
    "        break\n",
    "        \n",
    "print(err)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6c51b",
   "metadata": {},
   "source": [
    "How many epochs did you need to get MSE below 0.0075 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216517f",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34ab7b",
   "metadata": {},
   "source": [
    "#### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d2079",
   "metadata": {},
   "source": [
    "Implement gradient descent using pytorch. Start by just rewritting Problem 4 to use torch Tensors instead of numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860eeac",
   "metadata": {},
   "source": [
    "To convert frrom numpy arrays to torch tensors you can use ``torch.from_numpy()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb43ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7651e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7354, -0.6156, -0.6559,  ..., -1.9354, -0.2938, -0.5859],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "t_y = t.from_numpy(y)\n",
    "t_x = t.from_numpy(x)\n",
    "t_w = t.DoubleTensor([0,0,0])\n",
    "t_b = t.DoubleTensor([1.0])\n",
    "\n",
    "print(t_y - t.matmul(t_x,t_w.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec1b4f",
   "metadata": {},
   "source": [
    "Then use the automatic differentiation capabilities of Pytorch. To this end the variable with respect to which the gradient will be calculated, `t_w` and `t_b` in this case, must have attribute\n",
    "`requires_grad` set to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2540fd",
   "metadata": {},
   "source": [
    "The torch will automatically track any expression containing `t_w` and `t_b` and store its computational graph. The method `backward()` can be run on the final expression to back propagate the gradient. The gradient is then accesible as `t_w.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "665f89c4-1cf0-45ac-9238-52ab3849fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_w.requires_grad=True\n",
    "t_b.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8e448",
   "metadata": {},
   "source": [
    "Finally use  Pytorch  optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32bd788b-3e09-45d7-aa55-b5eb7f4bdd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2002,  0.5042, -0.1996], dtype=torch.float64, requires_grad=True) tensor([-1.0017], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "epochs=1000\n",
    "step=0.1\n",
    "treshold=0.0075\n",
    "optimizer = t.optim.SGD([t_w,t_b], lr=step)\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = ((t_y - t.matmul(t_x,t_w.T) - t_b)**2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(t_w,t_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6ca65-3028-470b-9bc7-877d6e546d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
